{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyVD3AVqg_mT"
      },
      "source": [
        "# **Model Version: 0.1**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2vh7_3cunTO9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bd27237-f0e4-45b4-cda1-46f9d35dd511"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c146e5-shGBG"
      },
      "source": [
        "### ***Imports***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "zWtXTHyDhL1k"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "import random\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import optim\n",
        "import argparse\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcpi_IJdhLAL"
      },
      "source": [
        "### **Models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "IA3-UdhFhCZN"
      },
      "outputs": [],
      "source": [
        "class TDNN(nn.Module):\n",
        "    \n",
        "    def __init__(\n",
        "                    self, \n",
        "                    input_dim=23, \n",
        "                    output_dim=512,\n",
        "                    context_size=5,\n",
        "                    stride=1,\n",
        "                    dilation=1,\n",
        "                    batch_norm=False,\n",
        "                    dropout_p=0.2\n",
        "                ):\n",
        "        '''\n",
        "        TDNN as defined by https://www.danielpovey.com/files/2015_interspeech_multisplice.pdf\n",
        "\n",
        "        Affine transformation not applied globally to all frames but smaller windows with local context\n",
        "\n",
        "        batch_norm: True to include batch normalisation after the non linearity\n",
        "        \n",
        "        Context size and dilation determine the frames selected\n",
        "        (although context size is not really defined in the traditional sense)\n",
        "        For example:\n",
        "            context size 5 and dilation 1 is equivalent to [-2,-1,0,1,2]\n",
        "            context size 3 and dilation 2 is equivalent to [-2, 0, 2]\n",
        "            context size 1 and dilation 1 is equivalent to [0]\n",
        "        '''\n",
        "        super(TDNN, self).__init__()\n",
        "        self.context_size = context_size\n",
        "        self.stride = stride\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.dilation = dilation\n",
        "        self.dropout_p = dropout_p\n",
        "        self.batch_norm = batch_norm\n",
        "      \n",
        "        self.kernel = nn.Linear(input_dim*context_size, output_dim)\n",
        "        self.nonlinearity = nn.ReLU()\n",
        "        if self.batch_norm:\n",
        "            self.bn = nn.BatchNorm1d(output_dim)\n",
        "        if self.dropout_p:\n",
        "            self.drop = nn.Dropout(p=self.dropout_p)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        input: size (batch, seq_len, input_features)\n",
        "        outpu: size (batch, new_seq_len, output_features)\n",
        "        '''\n",
        "        \n",
        "        _, _, d = x.shape\n",
        "        assert (d == self.input_dim), 'Input dimension was wrong. Expected ({}), got ({})'.format(self.input_dim, d)\n",
        "        x = x.unsqueeze(1)\n",
        "\n",
        "        # Unfold input into smaller temporal contexts\n",
        "        x = F.unfold(\n",
        "                        x, \n",
        "                        (self.context_size, self.input_dim), \n",
        "                        stride=(1,self.input_dim), \n",
        "                        dilation=(self.dilation,1)\n",
        "                    )\n",
        "\n",
        "        # N, output_dim*context_size, new_t = x.shape\n",
        "        x = x.transpose(1,2)\n",
        "        x = self.kernel(x.float())\n",
        "        x = self.nonlinearity(x)\n",
        "        \n",
        "        if self.dropout_p:\n",
        "            x = self.drop(x)\n",
        "\n",
        "        if self.batch_norm:\n",
        "            x = x.transpose(1,2)\n",
        "            x = self.bn(x)\n",
        "            x = x.transpose(1,2)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "GhX96WM1hbCD"
      },
      "outputs": [],
      "source": [
        "class X_vector(nn.Module):\n",
        "    def __init__(self, input_dim = 40, num_classes=10):\n",
        "        super(X_vector, self).__init__()\n",
        "        self.tdnn1 = TDNN(input_dim=input_dim, output_dim=1280, context_size=3, dilation=2,dropout_p=0.5)\n",
        "        self.tdnn2 = TDNN(input_dim=1280, output_dim=1280, context_size=1, dilation=1,dropout_p=0.5)\n",
        "        self.tdnn3 = TDNN(input_dim=1280, output_dim=1024, context_size=5, dilation=2,dropout_p=0.5)\n",
        "        self.tdnn4 = TDNN(input_dim=1024, output_dim=1024, context_size=1, dilation=1,dropout_p=0.5)\n",
        "        self.tdnn5 = TDNN(input_dim=1024, output_dim=768, context_size=2, dilation=1,dropout_p=0.5)\n",
        "        self.tdnn6 = TDNN(input_dim=768, output_dim=512, context_size=1, dilation=1,dropout_p=0.5)\n",
        "        self.tdnn7 = TDNN(input_dim=512, output_dim=256, context_size=1, dilation=3,dropout_p=0.5)\n",
        "        #### Frame levelPooling\n",
        "        self.segment8 = nn.Linear(512, 512)\n",
        "        self.segment9 = nn.Linear(512, 512)\n",
        "        self.output = nn.Linear(512, num_classes)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "    def forward(self, inputs):\n",
        "        tdnn1_out = self.tdnn1(inputs)\n",
        "        return tdnn1_out\n",
        "        tdnn2_out = self.tdnn2(tdnn1_out)\n",
        "        tdnn3_out = self.tdnn3(tdnn2_out)\n",
        "        tdnn4_out = self.tdnn4(tdnn3_out)\n",
        "        tdnn5_out = self.tdnn5(tdnn4_out)\n",
        "        tdnn6_out = self.tdnn6(tdnn5_out)\n",
        "        tdnn7_out = self.tdnn7(tdnn6_out)\n",
        "        ### Stat Pool\n",
        "        mean = torch.mean(tdnn7_out,1)\n",
        "        std = torch.std(tdnn7_out,1)\n",
        "        stat_pooling = torch.cat((mean,std),1)\n",
        "        segment8_out = self.segment8(stat_pooling)\n",
        "        x_vec = self.segment9(segment8_out)\n",
        "        predictions = self.softmax(self.output(x_vec))\n",
        "        return predictions,x_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "yiZdP05khlC3"
      },
      "outputs": [],
      "source": [
        "class X_vector(nn.Module):\n",
        "    def __init__(self, input_dim = 40, num_classes=10):\n",
        "        super(X_vector, self).__init__()\n",
        "        self.tdnn1 = TDNN(input_dim=input_dim, output_dim=1280, context_size=3, dilation=2,dropout_p=0.5)\n",
        "        self.tdnn2 = TDNN(input_dim=1280, output_dim=1280, context_size=1, dilation=1,dropout_p=0.5)\n",
        "        self.tdnn3 = TDNN(input_dim=1280, output_dim=1024, context_size=5, dilation=2,dropout_p=0.5)\n",
        "        self.tdnn4 = TDNN(input_dim=1024, output_dim=1024, context_size=1, dilation=1,dropout_p=0.5)\n",
        "        self.tdnn5 = TDNN(input_dim=1024, output_dim=768, context_size=2, dilation=1,dropout_p=0.5)\n",
        "        self.tdnn6 = TDNN(input_dim=768, output_dim=512, context_size=1, dilation=1,dropout_p=0.5)\n",
        "        self.tdnn7 = TDNN(input_dim=512, output_dim=256, context_size=1, dilation=3,dropout_p=0.5)\n",
        "        #### Frame levelPooling\n",
        "        self.segment8 = nn.Linear(512, 512)\n",
        "        self.segment9 = nn.Linear(512, 512)\n",
        "        self.output = nn.Linear(512, num_classes)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "    def forward(self, inputs):\n",
        "        tdnn1_out = self.tdnn1(inputs)\n",
        "        tdnn2_out = self.tdnn2(tdnn1_out)\n",
        "        tdnn3_out = self.tdnn3(tdnn2_out)\n",
        "        tdnn4_out = self.tdnn4(tdnn3_out)\n",
        "        tdnn5_out = self.tdnn5(tdnn4_out)\n",
        "        tdnn6_out = self.tdnn6(tdnn5_out)\n",
        "        tdnn7_out = self.tdnn7(tdnn6_out)\n",
        "        ### Stat Pool\n",
        "        \n",
        "        mean = torch.mean(tdnn7_out,1)\n",
        "        std = torch.std(tdnn7_out,1)\n",
        "        stat_pooling = torch.cat((mean,std),1)\n",
        "        segment8_out = self.segment8(stat_pooling)\n",
        "        x_vec = self.segment9(segment8_out)\n",
        "        predictions = self.output(x_vec)\n",
        "        return predictions,x_vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmFsaK-ijzkT"
      },
      "source": [
        "### Utiles "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "gV8XI7_4jUAH"
      },
      "outputs": [],
      "source": [
        "def load_wav(audio_filepath, sr, min_dur_sec=4):\n",
        "    audio_data,fs  = librosa.load(audio_filepath,sr=16000)\n",
        "    len_file = len(audio_data)\n",
        "    \n",
        "    if len_file <int(min_dur_sec*sr):\n",
        "        dummy=np.zeros((1,int(min_dur_sec*sr)-len_file))\n",
        "        extened_wav = np.concatenate((audio_data,dummy[0]))\n",
        "    else:\n",
        "        \n",
        "        extened_wav = audio_data\n",
        "    return extened_wav\n",
        "\n",
        "\n",
        "def lin_mel_from_wav(wav, hop_length, win_length, n_mels):\n",
        "    linear = librosa.feature.melspectrogram(wav, n_mels=n_mels, win_length=win_length, hop_length=hop_length) # linear spectrogram\n",
        "    return linear.T\n",
        "\n",
        "def lin_spectogram_from_wav(wav, hop_length, win_length, n_fft=512):\n",
        "    linear = librosa.stft(wav, n_fft=n_fft, win_length=win_length, hop_length=hop_length) # linear spectrogram\n",
        "    return linear.T\n",
        "\n",
        "\n",
        "def feature_extraction(filepath,sr=16000, min_dur_sec=4,win_length=400,hop_length=160, n_mels=40, spec_len=400,mode='train'):\n",
        "    audio_data = load_wav(filepath, sr=sr,min_dur_sec=min_dur_sec)\n",
        "    linear_spect = lin_spectogram_from_wav(audio_data, hop_length, win_length, n_fft=512)\n",
        "    mag, _ = librosa.magphase(linear_spect)  # magnitude\n",
        "    mag_T = mag.T\n",
        "    mu = np.mean(mag_T, 0, keepdims=True)\n",
        "    std = np.std(mag_T, 0, keepdims=True)\n",
        "    return (mag_T - mu) / (std + 1e-5)\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "def load_data(filepath,sr=16000, min_dur_sec=4,win_length=400,hop_length=160, n_mels=40, spec_len=400,mode='train'):\n",
        "    audio_data = load_wav(filepath, sr=sr,min_dur_sec=min_dur_sec)\n",
        "    #linear_spect = lin_spectogram_from_wav(audio_data, hop_length, win_length, n_mels)\n",
        "    linear_spect = lin_spectogram_from_wav(audio_data, hop_length, win_length, n_fft=512)\n",
        "    mag, _ = librosa.magphase(linear_spect)  # magnitude\n",
        "    mag_T = mag.T\n",
        "    \n",
        "    if mode=='train':\n",
        "        randtime = np.random.randint(0, mag_T.shape[1]-spec_len)\n",
        "        spec_mag = mag_T[:, randtime:randtime+spec_len]\n",
        "    else:\n",
        "        spec_mag = mag_T\n",
        "    \n",
        "    # preprocessing, subtract mean, divided by time-wise var\n",
        "    mu = np.mean(spec_mag, 0, keepdims=True)\n",
        "    std = np.std(spec_mag, 0, keepdims=True)\n",
        "    return (spec_mag - mu) / (std + 1e-5)\n",
        "    \n",
        "\n",
        "\n",
        "def load_npy_data(filepath,spec_len=400,mode='train'):\n",
        "    mag_T = np.load(filepath)\n",
        "    if mode=='train':\n",
        "        randtime = np.random.randint(0, mag_T.shape[1]-spec_len)\n",
        "        spec_mag = mag_T[:, randtime:randtime+spec_len]\n",
        "    else:\n",
        "        spec_mag = mag_T\n",
        "    return spec_mag\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "def speech_collate(batch):\n",
        "    targets = []\n",
        "    specs = []\n",
        "    for sample in batch:\n",
        "        specs.append(sample['features'])\n",
        "        targets.append((sample['labels']))\n",
        "    return specs, targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZTJukZ0j4Nr"
      },
      "source": [
        "## SpeechDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "gjvylivejY_y"
      },
      "outputs": [],
      "source": [
        "class SpeechDataGenerator():\n",
        "    \"\"\"Speech dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, manifest, mode):\n",
        "        \"\"\"\n",
        "        Read the textfile and get the paths\n",
        "        \"\"\"\n",
        "        \n",
        "        # [line.rstrip('\\n').split(' ')[0]\n",
        "        self.mode=mode\n",
        "        self.audio_links = [\" \".join(line.rstrip('\\n').split(' ')[:-1]) for line in open(manifest)]\n",
        "        self.labels = [int(line.rstrip('\\n').split(' ')[-1]) for line in open(manifest)]\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_links)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio_link =self.audio_links[idx]\n",
        "        class_id = self.labels[idx]\n",
        "        #lang_label=lang_id[self.audio_links[idx].split('/')[-2]]\n",
        "        spec = load_data(audio_link,mode=self.mode)\n",
        "        sample = {'features': torch.from_numpy(np.ascontiguousarray(spec)), 'labels': torch.from_numpy(np.ascontiguousarray(class_id))}\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZM6lFNOlwL6"
      },
      "source": [
        "## Train X Vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "idauk9yWlyJs"
      },
      "outputs": [],
      "source": [
        "torch.multiprocessing.set_sharing_strategy('file_system')\n",
        "\n",
        "########## Argument parser\n",
        "\n",
        "training_filepath = '/content/drive/My Drive/10_lang_reupload/train_10_lang.txt'\n",
        "testing_filepath = '/content/drive/My Drive/10_lang_reupload/test_10_lang.txt'\n",
        "validation_filepath = '/content/drive/My Drive/10_lang_reupload/validation_10_lang.txt'\n",
        "input_dim = 257\n",
        "num_classes = 10\n",
        "lamda_val = 0.5\n",
        "batch_size = 100\n",
        "use_gpu = True\n",
        "num_epochs = 100\n",
        "\n",
        "### Data related\n",
        "dataset_train = SpeechDataGenerator(manifest=training_filepath, mode='train')\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=speech_collate)\n",
        "\n",
        "dataset_val = SpeechDataGenerator(manifest=validation_filepath, mode='train')\n",
        "dataloader_val = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=speech_collate)\n",
        "\n",
        "dataset_test = SpeechDataGenerator(manifest=testing_filepath, mode='test')\n",
        "dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True, collate_fn=speech_collate)\n",
        "\n",
        "## Model related\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "model = X_vector(input_dim, num_classes).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0, betas=(0.9, 0.98), eps=1e-9)\n",
        "loss_fun = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "def train(dataloader_train, epoch):\n",
        "    train_loss_list = []\n",
        "    full_preds = []\n",
        "    full_gts = []\n",
        "    model.train()\n",
        "\n",
        "    print(\"batch: \")\n",
        "    for i_batch, sample_batched in enumerate(dataloader_train):\n",
        "        print(str(i_batch+1)+\" \", end='')\n",
        "\n",
        "        features = torch.from_numpy(np.asarray([torch_tensor.numpy().T for torch_tensor in sample_batched[0]])).float()\n",
        "        labels = torch.from_numpy(np.asarray([torch_tensor[0].numpy() for torch_tensor in sample_batched[1]]))\n",
        "        labels = labels.long()\n",
        "        features, labels = features.to(device), labels.to(device)\n",
        "        features.requires_grad = True\n",
        "        optimizer.zero_grad()\n",
        "        pred_logits, x_vec = model(features)\n",
        "        #### CE loss\n",
        "        loss = loss_fun(pred_logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss_list.append(loss.item())\n",
        "        # train_acc_list.append(accuracy)\n",
        "        # if i_batch%10==0:\n",
        "        #    print('Loss {} after {} iteration'.format(np.mean(np.asarray(train_loss_list)),i_batch))\n",
        "\n",
        "        predictions = np.argmax(pred_logits.detach().cpu().numpy(), axis=1)\n",
        "        for pred in predictions:\n",
        "            full_preds.append(pred)\n",
        "        for lab in labels.detach().cpu().numpy():\n",
        "            full_gts.append(lab)\n",
        "\n",
        "    mean_acc = accuracy_score(full_gts, full_preds)\n",
        "    mean_loss = np.mean(np.asarray(train_loss_list))\n",
        "    print('Total training loss {} and training Accuracy {} after {} epochs'.format(mean_loss, mean_acc, epoch))\n",
        "\n",
        "    folder_path = '/content/drive/My Drive/saved_models'\n",
        "    model_save_path = f'{folder_path}/saved_model_{i_batch}.pth'\n",
        "    state_dict = {\n",
        "        'model': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "        'epoch': num_epochs\n",
        "    }\n",
        "    torch.save(state_dict, model_save_path)\n",
        "\n",
        "\n",
        "def validation(dataloader_val, epoch):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_loss_list = []\n",
        "        full_preds = []\n",
        "        full_gts = []\n",
        "        for i_batch, sample_batched in enumerate(dataloader_val):\n",
        "            features = torch.from_numpy(\n",
        "                np.asarray([torch_tensor.numpy().T for torch_tensor in sample_batched[0]])).float()\n",
        "            labels = torch.from_numpy(np.asarray([torch_tensor[0].numpy() for torch_tensor in sample_batched[1]]))\n",
        "            labels = labels.long()\n",
        "            features, labels = features.to(device), labels.to(device)\n",
        "            pred_logits, x_vec = model(features)\n",
        "            #### CE loss\n",
        "            loss = loss_fun(pred_logits, labels)\n",
        "            val_loss_list.append(loss.item())\n",
        "            # train_acc_list.append(accuracy)\n",
        "            predictions = np.argmax(pred_logits.detach().cpu().numpy(), axis=1)\n",
        "            for pred in predictions:\n",
        "                full_preds.append(pred)\n",
        "            for lab in labels.detach().cpu().numpy():\n",
        "                full_gts.append(lab)\n",
        "\n",
        "        mean_acc = accuracy_score(full_gts, full_preds)\n",
        "        mean_loss = np.mean(np.asarray(val_loss_list))\n",
        "        print('Total validation loss {} and Validation accuracy {} after {} epochs'.format(mean_loss, mean_acc, epoch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qi0DAtGGncLE",
        "outputId": "7b5064b3-5b89-44d7-a5b0-b91ba51331be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 2.2551025458744594 and training Accuracy 0.1218077474892396 after 0 epochs\n",
            "Total validation loss 2.248920624596732 and Validation accuracy 0.1400286944045911 after 0 epochs\n",
            "2\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 2.207627180644444 and training Accuracy 0.1466284074605452 after 1 epochs\n",
            "Total validation loss 2.2061667374202183 and Validation accuracy 0.1473457675753228 after 1 epochs\n",
            "3\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 2.175761944907052 and training Accuracy 0.16685796269727404 after 2 epochs\n",
            "Total validation loss 2.209117388725281 and Validation accuracy 0.13773314203730272 after 2 epochs\n",
            "4\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 2.1252242139407564 and training Accuracy 0.1896700143472023 after 3 epochs\n",
            "Total validation loss 2.0407844764845713 and Validation accuracy 0.21434720229555237 after 3 epochs\n",
            "5\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 1.9887996894972666 and training Accuracy 0.2543758967001435 after 4 epochs\n",
            "Total validation loss 2.015996854645865 and Validation accuracy 0.29899569583931135 after 4 epochs\n",
            "6\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 1.871391601221902 and training Accuracy 0.3236728837876614 after 5 epochs\n",
            "Total validation loss 1.9176026054791042 and Validation accuracy 0.29655667144906744 after 5 epochs\n",
            "7\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 1.7951175877026149 and training Accuracy 0.3581061692969871 after 6 epochs\n",
            "Total validation loss 1.8854938183512007 and Validation accuracy 0.31076040172166425 after 6 epochs\n",
            "8\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 1.700089258807046 and training Accuracy 0.4067431850789096 after 7 epochs\n",
            "Total validation loss 1.9285872851099286 and Validation accuracy 0.2956958393113343 after 7 epochs\n",
            "9\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 1.5930723530905588 and training Accuracy 0.4434720229555237 after 8 epochs\n",
            "Total validation loss 1.7063531586102076 and Validation accuracy 0.3814921090387374 after 8 epochs\n",
            "10\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 1.5432776519230433 and training Accuracy 0.4644189383070301 after 9 epochs\n",
            "Total validation loss 1.5951058523995536 and Validation accuracy 0.43242467718794836 after 9 epochs\n",
            "11\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 1.4014084850038802 and training Accuracy 0.5172166427546628 after 10 epochs\n",
            "Total validation loss 1.6309724807739259 and Validation accuracy 0.415351506456241 after 10 epochs\n",
            "12\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 1.3277471099581037 and training Accuracy 0.5464849354375897 after 11 epochs\n",
            "Total validation loss 1.5055206741605487 and Validation accuracy 0.48923959827833574 after 11 epochs\n",
            "13\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 1.229982832499913 and training Accuracy 0.5857962697274032 after 12 epochs\n",
            "Total validation loss 1.3727355480194092 and Validation accuracy 0.5473457675753228 after 12 epochs\n",
            "14\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 1.1547855649675642 and training Accuracy 0.6055954088952654 after 13 epochs\n",
            "Total validation loss 1.2042011192866735 and Validation accuracy 0.5779053084648493 after 13 epochs\n",
            "15\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 1.1100913669381822 and training Accuracy 0.6299856527977045 after 14 epochs\n",
            "Total validation loss 1.3070157323564802 and Validation accuracy 0.5311334289813486 after 14 epochs\n",
            "16\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 1.0378074884414672 and training Accuracy 0.6507890961262554 after 15 epochs\n",
            "Total validation loss 1.1375200305666242 and Validation accuracy 0.6464849354375897 after 15 epochs\n",
            "17\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.9710859068802424 and training Accuracy 0.6748923959827834 after 16 epochs\n",
            "Total validation loss 1.0155265816620418 and Validation accuracy 0.6807747489239598 after 16 epochs\n",
            "18\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.9447521618434361 and training Accuracy 0.6840746054519369 after 17 epochs\n",
            "Total validation loss 1.0484266740935189 and Validation accuracy 0.6661406025824964 after 17 epochs\n",
            "19\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.8646420759814126 and training Accuracy 0.7073170731707317 after 18 epochs\n",
            "Total validation loss 0.8884537671293531 and Validation accuracy 0.7124820659971306 after 18 epochs\n",
            "20\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.844002205984933 and training Accuracy 0.7137733142037302 after 19 epochs\n",
            "Total validation loss 0.9424978699002947 and Validation accuracy 0.6902439024390243 after 19 epochs\n",
            "21\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.794952278477805 and training Accuracy 0.7329985652797705 after 20 epochs\n",
            "Total validation loss 0.9663484147616795 and Validation accuracy 0.6639885222381635 after 20 epochs\n",
            "22\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.8023486128875188 and training Accuracy 0.7354375896700144 after 21 epochs\n",
            "Total validation loss 0.9818218180111477 and Validation accuracy 0.646054519368723 after 21 epochs\n",
            "23\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.7209684486900058 and training Accuracy 0.7611190817790531 after 22 epochs\n",
            "Total validation loss 0.8033062917845589 and Validation accuracy 0.7185078909612626 after 22 epochs\n",
            "24\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.7037955399070467 and training Accuracy 0.7637015781922525 after 23 epochs\n",
            "Total validation loss 0.7357848789010729 and Validation accuracy 0.7738880918220947 after 23 epochs\n",
            "25\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.6845902906996864 and training Accuracy 0.7718794835007173 after 24 epochs\n",
            "Total validation loss 0.7660505950450898 and Validation accuracy 0.7621233859397417 after 24 epochs\n",
            "26\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.6624216786452702 and training Accuracy 0.7757532281205165 after 25 epochs\n",
            "Total validation loss 0.7225801272051675 and Validation accuracy 0.7704447632711621 after 25 epochs\n",
            "27\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.640227267571858 and training Accuracy 0.7912482065997131 after 26 epochs\n",
            "Total validation loss 0.7529299437999726 and Validation accuracy 0.7621233859397417 after 26 epochs\n",
            "28\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.5702484841857638 and training Accuracy 0.8098995695839312 after 27 epochs\n",
            "Total validation loss 0.7587131542818887 and Validation accuracy 0.7350071736011478 after 27 epochs\n",
            "29\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.6046667886631829 and training Accuracy 0.8015781922525108 after 28 epochs\n",
            "Total validation loss 0.6535108715295792 and Validation accuracy 0.7863701578192253 after 28 epochs\n",
            "30\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.5396104846681867 and training Accuracy 0.8205164992826399 after 29 epochs\n",
            "Total validation loss 0.6452698673520769 and Validation accuracy 0.7806312769010043 after 29 epochs\n",
            "31\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 "
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    print(epoch+1)\n",
        "    train(dataloader_train, epoch)\n",
        "    validation(dataloader_val, epoch)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_path = '/content/drive/My Drive/saved_final_model_epoch3.pth'\n",
        "state_dict = torch.load(model_save_path)\n",
        "model.load_state_dict(state_dict['model'])\n",
        "optimizer.load_state_dict(state_dict['optimizer'])\n",
        "epoch = state_dict['epoch']"
      ],
      "metadata": {
        "id": "8StDAelMXsq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(30, num_epochs):\n",
        "    print(epoch+1)\n",
        "    train(dataloader_train, epoch)\n",
        "    validation(dataloader_val, epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xkz4LZSlXs7m",
        "outputId": "9ae4f0a6-2788-432a-f510-64d9dad8521d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "31\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.48038682213851386 and training Accuracy 0.8461979913916786 after 30 epochs\n",
            "Total validation loss 0.6831744841166905 and Validation accuracy 0.7773314203730273 after 30 epochs\n",
            "32\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.4446802628891809 and training Accuracy 0.8592539454806313 after 31 epochs\n",
            "Total validation loss 0.6788057110139302 and Validation accuracy 0.7747489239598279 after 31 epochs\n",
            "33\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.41189250626734325 and training Accuracy 0.8692969870875179 after 32 epochs\n",
            "Total validation loss 0.5746541138206209 and Validation accuracy 0.8094691535150645 after 32 epochs\n",
            "34\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.4323576673865318 and training Accuracy 0.8622668579626973 after 33 epochs\n",
            "Total validation loss 0.38228935322591234 and Validation accuracy 0.8985652797704448 after 33 epochs\n",
            "35\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.3701417105538504 and training Accuracy 0.878909612625538 after 34 epochs\n",
            "Total validation loss 0.38137374435152327 and Validation accuracy 0.8949784791965567 after 34 epochs\n",
            "36\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.35853018377508433 and training Accuracy 0.8901004304160689 after 35 epochs\n",
            "Total validation loss 0.5121559905154365 and Validation accuracy 0.8489239598278335 after 35 epochs\n",
            "37\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.31620077299220223 and training Accuracy 0.8974175035868006 after 36 epochs\n",
            "Total validation loss 0.46245534121990206 and Validation accuracy 0.8449067431850789 after 36 epochs\n",
            "38\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.3231738729136331 and training Accuracy 0.8952654232424677 after 37 epochs\n",
            "Total validation loss 0.3541450896433422 and Validation accuracy 0.8923959827833573 after 37 epochs\n",
            "39\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.2890621121440615 and training Accuracy 0.9070301291248206 after 38 epochs\n",
            "Total validation loss 0.4344593580280032 and Validation accuracy 0.8691535150645624 after 38 epochs\n",
            "40\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.29424715020826886 and training Accuracy 0.9047345767575323 after 39 epochs\n",
            "Total validation loss 0.38123540920870647 and Validation accuracy 0.8888091822094691 after 39 epochs\n",
            "41\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.25319524833134244 and training Accuracy 0.9170731707317074 after 40 epochs\n",
            "Total validation loss 0.3705396479793957 and Validation accuracy 0.8826398852223817 after 40 epochs\n",
            "42\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.27086121163197924 and training Accuracy 0.9119081779053084 after 41 epochs\n",
            "Total validation loss 0.37056941517761777 and Validation accuracy 0.8903873744619799 after 41 epochs\n",
            "43\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.2573793166450092 and training Accuracy 0.9137733142037303 after 42 epochs\n",
            "Total validation loss 0.31936139230217253 and Validation accuracy 0.9040172166427547 after 42 epochs\n",
            "44\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.25731309652328493 and training Accuracy 0.9143472022955523 after 43 epochs\n",
            "Total validation loss 0.42053716693605697 and Validation accuracy 0.861119081779053 after 43 epochs\n",
            "45\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.23020025012748582 and training Accuracy 0.9243902439024391 after 44 epochs\n",
            "Total validation loss 0.33263156775917324 and Validation accuracy 0.8885222381635581 after 44 epochs\n",
            "46\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.21640149993555888 and training Accuracy 0.9298421807747489 after 45 epochs\n",
            "Total validation loss 0.3543440791113036 and Validation accuracy 0.8832137733142037 after 45 epochs\n",
            "47\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.24133389783757073 and training Accuracy 0.9276901004304161 after 46 epochs\n",
            "Total validation loss 0.400910404750279 and Validation accuracy 0.8677187948350071 after 46 epochs\n",
            "48\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.231935453414917 and training Accuracy 0.927116212338594 after 47 epochs\n",
            "Total validation loss 0.3897701186793191 and Validation accuracy 0.893687230989957 after 47 epochs\n",
            "49\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.22046425161617142 and training Accuracy 0.9275466284074605 after 48 epochs\n",
            "Total validation loss 0.29413288831710815 and Validation accuracy 0.9196556671449068 after 48 epochs\n",
            "50\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.1967248293438128 and training Accuracy 0.9368723098995696 after 49 epochs\n",
            "Total validation loss 0.34496158084699086 and Validation accuracy 0.8822094691535151 after 49 epochs\n",
            "51\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.2298958542091506 and training Accuracy 0.9249641319942611 after 50 epochs\n",
            "Total validation loss 0.28867552557161874 and Validation accuracy 0.9203730272596844 after 50 epochs\n",
            "52\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.20345482943313462 and training Accuracy 0.9373027259684361 after 51 epochs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_path = '/content/drive/My Drive/saved_model_50(92acc).pth'\n",
        "state_dict = torch.load(model_save_path)\n",
        "model.load_state_dict(state_dict['model'])\n",
        "optimizer.load_state_dict(state_dict['optimizer'])\n",
        "epoch = state_dict['epoch']"
      ],
      "metadata": {
        "id": "nDUtlJleAf5G"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, num_epochs):\n",
        "    print(epoch+1)\n",
        "    train(dataloader_train, epoch)\n",
        "    validation(dataloader_val, epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mfl0dK5t_veR",
        "outputId": "cd6139ac-4970-4da4-f71c-c4811ec0f3c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.5924872773034232 and training Accuracy 0.8107409534750144 after 1 epochs\n",
            "Total validation loss 0.7045712181500026 and Validation accuracy 0.7798678920160828 after 1 epochs\n",
            "3\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.5345376836402076 and training Accuracy 0.829408385985066 after 2 epochs\n",
            "Total validation loss 0.5680493061031614 and Validation accuracy 0.8334290637564618 after 2 epochs\n",
            "4\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.4982466261301722 and training Accuracy 0.8433371625502585 after 3 epochs\n",
            "Total validation loss 0.8051496395042964 and Validation accuracy 0.7375071797817346 after 3 epochs\n",
            "5\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.45478360780647825 and training Accuracy 0.8558299827685238 after 4 epochs\n",
            "Total validation loss 0.5545947560242244 and Validation accuracy 0.8502297530155083 after 4 epochs\n",
            "6\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.41283448657819205 and training Accuracy 0.8657380815623205 after 5 epochs\n",
            "Total validation loss 0.5037783959082195 and Validation accuracy 0.8589890867317633 after 5 epochs\n",
            "7\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.40847179144620893 and training Accuracy 0.8667432510051695 after 6 epochs\n",
            "Total validation loss 0.5361617394856044 and Validation accuracy 0.8296955772544514 after 6 epochs\n",
            "8\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.3817777244108064 and training Accuracy 0.8788052843193567 after 7 epochs\n",
            "Total validation loss 0.4614356407097408 and Validation accuracy 0.878661688684664 after 7 epochs\n",
            "9\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.40691825385604585 and training Accuracy 0.8706203331418725 after 8 epochs\n",
            "Total validation loss 0.5337126323154995 and Validation accuracy 0.8525272831705916 after 8 epochs\n",
            "10\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.3608196499092238 and training Accuracy 0.8869902354968409 after 9 epochs\n",
            "Total validation loss 0.4394620605877468 and Validation accuracy 0.8772257323377369 after 9 epochs\n",
            "11\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.36106870195695334 and training Accuracy 0.8855542791499138 after 10 epochs\n",
            "Total validation loss 0.5061129514660154 and Validation accuracy 0.8460654796094199 after 10 epochs\n",
            "12\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.34440790116786957 and training Accuracy 0.8902929350947731 after 11 epochs\n",
            "Total validation loss 0.4107743220669883 and Validation accuracy 0.9014933946008041 after 11 epochs\n",
            "13\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.3093212710959571 and training Accuracy 0.9002010338885698 after 12 epochs\n",
            "Total validation loss 0.41858570533139366 and Validation accuracy 0.8780873061458931 after 12 epochs\n",
            "14\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.3133236836109843 and training Accuracy 0.9009190120620333 after 13 epochs\n",
            "Total validation loss 0.4216420386518751 and Validation accuracy 0.8934520390580126 after 13 epochs\n",
            "15\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.32413181023938314 and training Accuracy 0.8968983342906376 after 14 epochs\n",
            "Total validation loss 0.45430314498288293 and Validation accuracy 0.8696151636990236 after 14 epochs\n",
            "16\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.3071615397930145 and training Accuracy 0.9052268811028145 after 15 epochs\n",
            "Total validation loss 0.46715091041156226 and Validation accuracy 0.8700459506031016 after 15 epochs\n",
            "17\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.2923376240900585 and training Accuracy 0.9076680068925904 after 16 epochs\n",
            "Total validation loss 0.3827425309589931 and Validation accuracy 0.8944572085008615 after 16 epochs\n",
            "18\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.3335105472377368 and training Accuracy 0.9003446295232624 after 17 epochs\n",
            "Total validation loss 0.4090785343732153 and Validation accuracy 0.8963239517518667 after 17 epochs\n",
            "19\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.2787725782820157 and training Accuracy 0.9134118322802987 after 18 epochs\n",
            "Total validation loss 0.3752356401511601 and Validation accuracy 0.8954623779437105 after 18 epochs\n",
            "20\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.30591699321355137 and training Accuracy 0.9060884549109707 after 19 epochs\n",
            "Total validation loss 0.3805490397981235 and Validation accuracy 0.9016369902354968 after 19 epochs\n",
            "21\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.29339734762907027 and training Accuracy 0.9105399195864445 after 20 epochs\n",
            "Total validation loss 0.33469871836049214 and Validation accuracy 0.9237507179781734 after 20 epochs\n",
            "22\n",
            "batch: \n",
            "1 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.26602220386266706 and training Accuracy 0.9172889144170018 after 21 epochs\n",
            "Total validation loss 0.35849364421197344 and Validation accuracy 0.9079551981619759 after 21 epochs\n",
            "23\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.28483358664172037 and training Accuracy 0.9125502584721424 after 22 epochs\n",
            "Total validation loss 0.34767405007566726 and Validation accuracy 0.9201608271108558 after 22 epochs\n",
            "24\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.2755006214869874 and training Accuracy 0.9164273406088455 after 23 epochs\n",
            "Total validation loss 0.39536125319344656 and Validation accuracy 0.905944859276278 after 23 epochs\n",
            "25\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total validation loss 0.2626085155776569 and Validation accuracy 0.951751866743251 after 24 epochs\n",
            "26\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.2539649644068309 and training Accuracy 0.9217403790924756 after 25 epochs\n",
            "Total validation loss 0.305573114327022 and Validation accuracy 0.9338024124066628 after 25 epochs\n",
            "27\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.2610734485089779 and training Accuracy 0.9226019529006318 after 26 epochs\n",
            "Total validation loss 0.3972128923450198 and Validation accuracy 0.8999138426191844 after 26 epochs\n",
            "28\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.2505417530025755 and training Accuracy 0.9231763354394027 after 27 epochs\n",
            "Total validation loss 0.3580314689448902 and Validation accuracy 0.9070936243538197 after 27 epochs\n",
            "29\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.23452238515019416 and training Accuracy 0.9300689259046525 after 28 epochs\n",
            "Total validation loss 0.2866722419857979 and Validation accuracy 0.9348075818495117 after 28 epochs\n",
            "30\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.26631648923669543 and training Accuracy 0.9204480183802413 after 29 epochs\n",
            "Total validation loss 0.28689335265329907 and Validation accuracy 0.9485927627800115 after 29 epochs\n",
            "31\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.2138487774346556 and training Accuracy 0.9336588167719702 after 30 epochs\n",
            "Total validation loss 0.2924287389431681 and Validation accuracy 0.9306433084434234 after 30 epochs\n",
            "32\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.23681807975683894 and training Accuracy 0.9339460080413555 after 31 epochs\n",
            "Total validation loss 0.2350719711610249 and Validation accuracy 0.9629523262492821 after 31 epochs\n",
            "33\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.21239839898688453 and training Accuracy 0.938110281447444 after 32 epochs\n",
            "Total validation loss 0.28548544027975625 and Validation accuracy 0.9356691556576681 after 32 epochs\n",
            "34\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.23931491768785884 and training Accuracy 0.9303561171740379 after 33 epochs\n",
            "Total validation loss 0.3190611415675708 and Validation accuracy 0.9250430786904078 after 33 epochs\n",
            "35\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.21743252224155835 and training Accuracy 0.9349511774842045 after 34 epochs\n",
            "Total validation loss 0.2697951412626675 and Validation accuracy 0.9452900631820793 after 34 epochs\n",
            "36\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.19827686196991376 and training Accuracy 0.9411257897759908 after 35 epochs\n",
            "Total validation loss 0.29843832105398177 and Validation accuracy 0.9230327398047099 after 35 epochs\n",
            "37\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.24141440806644304 and training Accuracy 0.9323664560597358 after 36 epochs\n",
            "Total validation loss 0.32711748714957917 and Validation accuracy 0.9303561171740379 after 36 epochs\n",
            "38\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.20281752816268375 and training Accuracy 0.94055140723722 after 37 epochs\n",
            "Total validation loss 0.3244179666042328 and Validation accuracy 0.9125502584721424 after 37 epochs\n",
            "39\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.20030565687588284 and training Accuracy 0.941556576680069 after 38 epochs\n",
            "Total validation loss 0.3163452450718198 and Validation accuracy 0.9115450890292935 after 38 epochs\n",
            "40\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.2018909356955971 and training Accuracy 0.9422745548535324 after 39 epochs\n",
            "Total validation loss 0.2859739686761584 and Validation accuracy 0.933228029867892 after 39 epochs\n",
            "41\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.21238624283245633 and training Accuracy 0.9391154508902929 after 40 epochs\n",
            "Total validation loss 0.24616246074438095 and Validation accuracy 0.9422745548535324 after 40 epochs\n",
            "42\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.21864043857370105 and training Accuracy 0.9375358989086732 after 41 epochs\n",
            "Total validation loss 0.31489415317773817 and Validation accuracy 0.9326536473291211 after 41 epochs\n",
            "43\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.20601384373647827 and training Accuracy 0.9414129810453762 after 42 epochs\n",
            "Total validation loss 0.31019403700317655 and Validation accuracy 0.9481619758759334 after 42 epochs\n",
            "44\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.1978605676974569 and training Accuracy 0.944428489373923 after 43 epochs\n",
            "Total validation loss 0.22874475345015527 and Validation accuracy 0.9612291786329695 after 43 epochs\n",
            "45\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.16564909842397485 and training Accuracy 0.9530442274554853 after 44 epochs\n",
            "Total validation loss 0.2718749259199415 and Validation accuracy 0.9294945433658817 after 44 epochs\n",
            "46\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.2212800566639219 and training Accuracy 0.9402642159678346 after 45 epochs\n",
            "Total validation loss 0.3432584922228541 and Validation accuracy 0.9135554279149913 after 45 epochs\n",
            "47\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.21993651528443608 and training Accuracy 0.9379666858127513 after 46 epochs\n",
            "Total validation loss 0.28416079240185876 and Validation accuracy 0.9388282596209075 after 46 epochs\n",
            "48\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.1723687824926206 and training Accuracy 0.951751866743251 after 47 epochs\n",
            "Total validation loss 0.23610235803893634 and Validation accuracy 0.94055140723722 after 47 epochs\n",
            "49\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.2292685689670699 and training Accuracy 0.9343767949454337 after 48 epochs\n",
            "Total validation loss 0.30405277269227166 and Validation accuracy 0.9345203905801264 after 48 epochs\n",
            "50\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.17368400080927782 and training Accuracy 0.9514646754738656 after 49 epochs\n",
            "Total validation loss 0.23860961198806763 and Validation accuracy 0.9516082711085583 after 49 epochs\n",
            "51\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.18376379263188158 and training Accuracy 0.9480183802412406 after 50 epochs\n",
            "Total validation loss 0.22984736093452998 and Validation accuracy 0.960080413555428 after 50 epochs\n",
            "52\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.1846176013882671 and training Accuracy 0.9490235496840896 after 51 epochs\n",
            "Total validation loss 0.28962717183998654 and Validation accuracy 0.9276278001148766 after 51 epochs\n",
            "53\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.18060213439166545 and training Accuracy 0.9467260195290064 after 52 epochs\n",
            "Total validation loss 0.2788878811257226 and Validation accuracy 0.9319356691556576 after 52 epochs\n",
            "54\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.16979814978050334 and training Accuracy 0.9514646754738656 after 53 epochs\n",
            "Total validation loss 0.2105806328356266 and Validation accuracy 0.9612291786329695 after 53 epochs\n",
            "55\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.17288688298846994 and training Accuracy 0.9524698449167145 after 54 epochs\n",
            "Total validation loss 0.30062988017286574 and Validation accuracy 0.9290637564618036 after 54 epochs\n",
            "56\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.20164468793996743 and training Accuracy 0.9487363584147042 after 55 epochs\n",
            "Total validation loss 0.25515589096716473 and Validation accuracy 0.9504595060310167 after 55 epochs\n",
            "57\n",
            "batch: \n",
            "1 2 3 4 5 6 7 8 9 10 11 12 13 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Total training loss 0.1802743645784046 and training Accuracy 0.9474439977024699 after 56 epochs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_path_files(root_folder, output_folder):\n",
        "\n",
        "    # Create output folder if it doesn't exist\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    \n",
        "    train_file = os.path.join(output_folder, 'train_10_lang.txt')\n",
        "    test_file = os.path.join(output_folder, 'test_10_lang.txt')\n",
        "    validation_file = os.path.join(output_folder, 'validation_10_lang.txt')\n",
        "\n",
        "    train_subfolders = []\n",
        "    test_subfolders = []\n",
        "    validation_subfolders = []\n",
        "    counter_label = 0\n",
        "\n",
        "    lang_label_dict = {}\n",
        "\n",
        "    for folder_lang in os.listdir(root_folder):\n",
        "        sub_folder = os.path.join(root_folder, folder_lang)\n",
        "        if os.path.isdir(sub_folder):\n",
        "\n",
        "            for subsub_folder in os.listdir(sub_folder):\n",
        "                subsub_folder_path = os.path.join(sub_folder, subsub_folder)\n",
        "                if os.path.isdir(subsub_folder_path):\n",
        "\n",
        "                    if subsub_folder == 'train':\n",
        "                        for root, _, files in os.walk(subsub_folder_path):\n",
        "                            for file in files:\n",
        "                                file_path = os.path.join(root, file)\n",
        "                                path_label = file_path + ' ' + str(counter_label)\n",
        "                                train_subfolders.append(path_label)\n",
        "                    elif subsub_folder == 'test':\n",
        "                        for root, _, files in os.walk(subsub_folder_path):\n",
        "                            for file in files:\n",
        "                                file_path = os.path.join(root, file)\n",
        "                                path_label = file_path + ' ' + str(counter_label)\n",
        "                                test_subfolders.append(path_label)\n",
        "                    elif subsub_folder == 'validate':\n",
        "                        for root, _, files in os.walk(subsub_folder_path):\n",
        "                            for file in files:\n",
        "                                file_path = os.path.join(root, file)\n",
        "                                path_label = file_path + ' ' + str(counter_label)\n",
        "                                validation_subfolders.append(path_label)\n",
        "        lang_label_dict[folder_lang] = counter_label\n",
        "        counter_label += 1\n",
        "\n",
        "    with open(train_file, 'w') as file:\n",
        "        file.write('\\n'.join(train_subfolders))\n",
        "\n",
        "    with open(test_file, 'w') as file:\n",
        "        file.write('\\n'.join(test_subfolders))\n",
        "\n",
        "    with open(validation_file, 'w') as file:\n",
        "        file.write('\\n'.join(validation_subfolders))\n",
        "\n",
        "    print(\"======================================\")\n",
        "    print(\"Txt files updated successfully!\")\n",
        "    print(f'Total labels: {counter_label}')\n",
        "    print(f'Lang dict: {lang_label_dict}')\n",
        "    print(\"======================================\")"
      ],
      "metadata": {
        "id": "gKTOPZrV_vw9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root_folder_path = '/content/drive/My Drive/10_lang_reupload/'\n",
        "output_folder_path = '/content/drive/My Drive/10_lang_reupload/'\n",
        "create_path_files(root_folder_path, output_folder_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSzNXWuC_yi4",
        "outputId": "9c3b24b2-ff47-4ef1-823e-663f994834f2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================\n",
            "Txt files updated successfully!\n",
            "Total labels: 10\n",
            "Lang dict: {'clips_ar': 0, 'clips_es': 1, 'clips_fa': 2, 'clips_fr': 3, 'clips_ha': 4, 'clips_it': 5, 'clips_ja': 6, 'clips_ru': 7, 'clips_th': 8, 'clips_zhCN': 9}\n",
            "======================================\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "c146e5-shGBG",
        "UmFsaK-ijzkT",
        "FZTJukZ0j4Nr"
      ],
      "gpuType": "T4"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}